# ü§ñ Integraci√≥n de RoBERTa en el Sistema RAG

Este documento explica c√≥mo usar **RoBERTa-base** como modelo de embeddings en el sistema RAG.

## üìã Tabla de Contenidos
- [Caracter√≠sticas](#caracter√≠sticas)
- [Instalaci√≥n](#instalaci√≥n)
- [Configuraci√≥n](#configuraci√≥n)
- [Uso](#uso)
- [Comparaci√≥n con SentenceTransformers](#comparaci√≥n-con-sentencetransformers)
- [Ejemplos de C√≥digo](#ejemplos-de-c√≥digo)
- [Consideraciones de Rendimiento](#consideraciones-de-rendimiento)

## ‚ú® Caracter√≠sticas

### RoBERTa vs BERT
RoBERTa (Robustly Optimized BERT Pretraining Approach) es una versi√≥n mejorada de BERT que:
- **Entrena m√°s tiempo** con batches m√°s grandes
- **Elimina NSP** (Next Sentence Prediction)
- **Usa m√°scaras din√°micas** que cambian en cada √©poca
- **Entrena con m√°s datos** (160GB vs 16GB de BERT)

### Ventajas en RAG
- **Mejor comprensi√≥n contextual**: Captura relaciones sem√°nticas m√°s profundas
- **Embeddings de alta calidad**: 768 dimensiones (base) o 1024 (large)
- **Multiling√ºe**: Soporta m√∫ltiples idiomas
- **Transfer learning**: Se adapta bien a dominios espec√≠ficos

## üöÄ Instalaci√≥n

### 1. Instalar dependencias
```bash
pip install -r requirements.txt
```

Las nuevas dependencias incluyen:
- `transformers>=4.30.0` - Biblioteca de Hugging Face
- `torch>=2.0.0` - PyTorch para el modelo

### 2. Configurar variables de entorno
Copia el archivo de ejemplo y ed√≠talo:
```bash
cp .env.example .env
```

Configura las variables para RoBERTa:
```bash
# Activar RoBERTa
USE_ROBERTA=true
EMBEDDING_MODEL=roberta-base

# Configuraci√≥n opcional
ROBERTA_DEVICE=cuda  # Si tienes GPU
ROBERTA_BATCH_SIZE=16  # Aumentar si tienes m√°s memoria
ROBERTA_MAX_LENGTH=512  # Longitud m√°xima de tokens
```

## ‚öôÔ∏è Configuraci√≥n

### Modelos Disponibles
| Modelo | Par√°metros | Dimensi√≥n | Uso Recomendado |
|--------|------------|-----------|-----------------|
| `roberta-base` | 125M | 768 | Balance velocidad/calidad |
| `roberta-large` | 355M | 1024 | M√°xima calidad |
| `distilroberta-base` | 82M | 768 | Mayor velocidad |
| `xlm-roberta-base` | 270M | 768 | Multiling√ºe |

### Configuraci√≥n en Python
```python
from src.config import settings

# Las configuraciones se cargan autom√°ticamente desde .env
print(f"Usando RoBERTa: {settings.use_roberta}")
print(f"Modelo: {settings.embedding_model}")
print(f"Device: {settings.roberta_device or 'auto'}")
```

## üìñ Uso

### 1. Procesamiento de Documentos con RoBERTa

```python
from src.document_processor_roberta import DocumentProcessorRoBERTa

# Inicializar procesador con RoBERTa
processor = DocumentProcessorRoBERTa(
    use_roberta=True,
    embedding_model="roberta-base",
    chunk_size=500,
    chunk_overlap=100,
    device="cuda"  # o "cpu"
)

# Procesar un texto
text = "Tu texto aqu√≠..."
chunks = processor.process_single_text(text)

# Procesar documentos Markdown
all_chunks = processor.process_documents(
    "ruta/a/documentos",
    file_pattern="*.md",
    show_progress=True
)
```

### 2. Vector Store con RoBERTa

```python
from src.vector_store_roberta import MongoVectorStoreRoBERTa

# Inicializar vector store
vector_store = MongoVectorStoreRoBERTa(
    mongodb_uri="mongodb://localhost:27017",
    database_name="rag_testing",
    collection_name="roberta_embeddings",
    use_roberta=True,
    embedding_model="roberta-base"
)

# Almacenar documentos
vector_store.add_documents(chunks)

# B√∫squeda por similitud
results = vector_store.similarity_search(
    query="¬øQu√© es machine learning?",
    top_k=5,
    similarity_threshold=0.7
)
```

### 3. Uso Directo de RoBERTa Embeddings

```python
from src.roberta_embeddings import RoBERTaEmbeddings

# Inicializar modelo
embeddings_model = RoBERTaEmbeddings(
    model_name="roberta-base",
    device="cuda",
    batch_size=8
)

# Generar embeddings para documentos
docs = ["Documento 1", "Documento 2", "Documento 3"]
doc_embeddings = embeddings_model.encode_documents(
    docs,
    normalize_embeddings=True,
    show_progress_bar=True
)

# Generar embeddings para queries
query = "Mi pregunta de b√∫squeda"
query_embedding = embeddings_model.encode_queries(query)

# Calcular similitud
similarities = embeddings_model.similarity(query_embedding, doc_embeddings)
```

## üìä Comparaci√≥n con SentenceTransformers

| Aspecto | RoBERTa-base | SentenceTransformers |
|---------|--------------|----------------------|
| **Dimensi√≥n de embeddings** | 768 | 384-768 |
| **Velocidad** | Medio | R√°pido |
| **Calidad sem√°ntica** | Excelente | Muy buena |
| **Uso de memoria** | ~500MB | ~100-400MB |
| **Comprensi√≥n contextual** | Superior | Buena |
| **Optimizado para similitud** | No | S√≠ |

### ¬øCu√°ndo usar cada uno?

**Usa RoBERTa cuando:**
- Necesitas m√°xima calidad en comprensi√≥n sem√°ntica
- Trabajas con textos complejos o t√©cnicos
- Tienes recursos computacionales disponibles (GPU)
- La precisi√≥n es m√°s importante que la velocidad

**Usa SentenceTransformers cuando:**
- Necesitas procesamiento r√°pido
- Trabajas con grandes vol√∫menes de datos
- Los recursos son limitados
- La tarea principal es b√∫squeda por similitud

## üíª Ejemplos de C√≥digo

### Ejemplo Completo: Sistema RAG con RoBERTa

```python
import os
from dotenv import load_dotenv
from src.document_processor_roberta import DocumentProcessorRoBERTa
from src.vector_store_roberta import MongoVectorStoreRoBERTa

# Cargar configuraci√≥n
load_dotenv()

def setup_rag_system():
    """Configura el sistema RAG completo con RoBERTa."""
    
    # 1. Procesar documentos
    processor = DocumentProcessorRoBERTa(
        use_roberta=True,
        embedding_model="roberta-base",
        chunk_size=500
    )
    
    chunks = processor.process_documents("./documents")
    
    # 2. Almacenar en MongoDB
    vector_store = MongoVectorStoreRoBERTa(
        mongodb_uri=os.getenv("MONGODB_URI"),
        database_name="rag_roberta",
        collection_name="documents",
        use_roberta=True
    )
    
    vector_store.add_documents(chunks)
    
    # 3. Funci√≥n de b√∫squeda
    def search(query: str, top_k: int = 5):
        results = vector_store.similarity_search(
            query=query,
            top_k=top_k,
            similarity_threshold=0.6
        )
        return results
    
    return search

# Usar el sistema
search_fn = setup_rag_system()
results = search_fn("¬øC√≥mo funciona RoBERTa?")
for result in results:
    print(f"[{result['similarity']:.3f}] {result['content'][:100]}...")
```

### B√∫squeda H√≠brida

```python
# Crear √≠ndice de texto para b√∫squeda h√≠brida
vector_store.create_text_index(["content", "metadata.title"])

# Realizar b√∫squeda h√≠brida
results = vector_store.hybrid_search(
    query="machine learning con transformers",
    text_search_field="content",
    top_k=10,
    similarity_weight=0.7,  # 70% peso para embeddings
    text_weight=0.3         # 30% peso para b√∫squeda de texto
)
```

## ‚ö° Consideraciones de Rendimiento

### Optimizaci√≥n de Memoria
```python
# Para GPUs con memoria limitada
processor = DocumentProcessorRoBERTa(
    use_roberta=True,
    batch_size=4,  # Reducir batch size
    device="cuda"
)
```

### Procesamiento en Lotes
```python
# Procesar documentos grandes en lotes
import numpy as np

docs = ["doc1", "doc2", ..., "doc1000"]
batch_size = 100

all_embeddings = []
for i in range(0, len(docs), batch_size):
    batch = docs[i:i+batch_size]
    embeddings = embeddings_model.encode_documents(batch)
    all_embeddings.append(embeddings)

final_embeddings = np.vstack(all_embeddings)
```

### Cache de Embeddings
```python
import pickle

# Guardar embeddings
with open('embeddings_cache.pkl', 'wb') as f:
    pickle.dump(embeddings, f)

# Cargar embeddings
with open('embeddings_cache.pkl', 'rb') as f:
    cached_embeddings = pickle.load(f)
```

## üß™ Testing

Ejecuta el script de ejemplo para probar todas las funcionalidades:

```bash
python example_roberta_usage.py
```

Este script incluye:
- Demo de procesamiento con RoBERTa
- Almacenamiento en MongoDB
- B√∫squedas por similitud
- Comparaci√≥n con SentenceTransformers
- Procesamiento de documentos Markdown

## üìà M√©tricas y Evaluaci√≥n

Para evaluar la calidad de los embeddings:

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Evaluar coherencia sem√°ntica
test_pairs = [
    ("machine learning", "aprendizaje autom√°tico"),
    ("deep learning", "redes neuronales profundas"),
    ("gato", "perro"),  # Relacionados pero diferentes
    ("computadora", "pizza")  # No relacionados
]

for text1, text2 in test_pairs:
    emb1 = embeddings_model.encode([text1])
    emb2 = embeddings_model.encode([text2])
    sim = cosine_similarity(emb1, emb2)[0][0]
    print(f"{text1} <-> {text2}: {sim:.3f}")
```

## üîß Troubleshooting

### Error: CUDA out of memory
```python
# Soluci√≥n 1: Reducir batch size
processor = DocumentProcessorRoBERTa(batch_size=2)

# Soluci√≥n 2: Usar CPU
processor = DocumentProcessorRoBERTa(device="cpu")

# Soluci√≥n 3: Usar modelo m√°s peque√±o
processor = DocumentProcessorRoBERTa(
    embedding_model="distilroberta-base"
)
```

### Error: Modelo descarga lenta
```python
# Usar mirror de Hugging Face
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

### Embeddings no normalizados
```python
# Siempre normalizar para cosine similarity
embeddings = embeddings_model.encode(
    texts,
    normalize_embeddings=True  # Importante!
)
```

## üìö Referencias

- [RoBERTa Paper](https://arxiv.org/abs/1907.11692)
- [Hugging Face RoBERTa](https://huggingface.co/roberta-base)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Transformers Library](https://huggingface.co/docs/transformers)

## ü§ù Soporte

Si tienes problemas o preguntas:
1. Revisa este README
2. Ejecuta el script de ejemplo: `python example_roberta_usage.py`
3. Verifica los logs en la consola
4. Aseg√∫rate de que MongoDB est√© ejecut√°ndose

---

**Nota:** RoBERTa requiere m√°s recursos computacionales que SentenceTransformers. Para producci√≥n, considera usar GPU o un servicio de inferencia como Hugging Face Inference API.
