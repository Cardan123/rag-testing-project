"""
Ejemplo de uso del sistema RAG con RoBERTa-base para embeddings.
Este script demuestra c√≥mo:
1. Procesar documentos con RoBERTa
2. Almacenar embeddings en MongoDB
3. Realizar b√∫squedas por similitud
4. Comparar con SentenceTransformers
"""

import os
from pathlib import Path
from dotenv import load_dotenv
import logging
import time
from typing import List, Dict, Any

# Importar las clases del sistema
from src.document_processor_roberta import DocumentProcessorRoBERTa
from src.vector_store_roberta import MongoVectorStoreRoBERTa
from src.config import settings

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Cargar variables de entorno
load_dotenv()


def demo_roberta_processing():
    """Demuestra el procesamiento de documentos con RoBERTa."""
    
    print("\n" + "="*60)
    print("DEMO: Procesamiento de Documentos con RoBERTa-base")
    print("="*60 + "\n")
    
    # 1. Inicializar el procesador con RoBERTa
    processor_roberta = DocumentProcessorRoBERTa(
        use_roberta=True,
        embedding_model="roberta-base",
        chunk_size=500,  # Chunks m√°s peque√±os para demostraci√≥n
        chunk_overlap=100,
        device=settings.roberta_device,
        batch_size=settings.roberta_batch_size
    )
    
    # 2. Procesar un texto de ejemplo
    sample_text = """
    La inteligencia artificial est√° transformando el mundo. Los modelos de lenguaje
    como RoBERTa han demostrado ser muy efectivos para tareas de comprensi√≥n del
    lenguaje natural. RoBERTa es una versi√≥n optimizada de BERT que utiliza m√°s
    datos de entrenamiento y t√©cnicas mejoradas.
    
    En el campo del procesamiento del lenguaje natural, los embeddings son
    representaciones vectoriales densas que capturan el significado sem√°ntico
    del texto. Estos embeddings permiten realizar b√∫squedas sem√°nticas eficientes
    y encontrar documentos relevantes basados en similitud.
    """
    
    print("Texto de ejemplo:")
    print("-" * 40)
    print(sample_text[:200] + "...")
    print("-" * 40 + "\n")
    
    # Procesar el texto
    chunks = processor_roberta.process_single_text(sample_text)
    
    print(f"‚úì Texto procesado en {len(chunks)} chunks")
    print(f"‚úì Dimensi√≥n de embeddings: {chunks[0]['embedding_dimension']}")
    print(f"‚úì Modelo usado: RoBERTa-base\n")
    
    return chunks


def demo_vector_store():
    """Demuestra el almacenamiento y b√∫squeda con MongoDB."""
    
    print("\n" + "="*60)
    print("DEMO: Vector Store con MongoDB y RoBERTa")
    print("="*60 + "\n")
    
    # 1. Inicializar el vector store
    vector_store = MongoVectorStoreRoBERTa(
        mongodb_uri=settings.mongodb_uri,
        database_name=settings.mongodb_database,
        collection_name="roberta_test_collection",
        use_roberta=True,
        embedding_model="roberta-base",
        device=settings.roberta_device
    )
    
    # 2. Limpiar colecci√≥n para la demo
    vector_store.clear_collection()
    print("‚úì Colecci√≥n limpiada\n")
    
    # 3. Crear y almacenar documentos de ejemplo
    documents = [
        {
            "title": "Introducci√≥n a Machine Learning",
            "content": """Machine Learning es una rama de la inteligencia artificial que permite
            a las computadoras aprender de los datos sin ser programadas expl√≠citamente.
            Los algoritmos de ML pueden identificar patrones y hacer predicciones."""
        },
        {
            "title": "Deep Learning y Redes Neuronales",
            "content": """Deep Learning utiliza redes neuronales artificiales con m√∫ltiples capas
            para aprender representaciones jer√°rquicas de los datos. Es especialmente efectivo
            en visi√≥n por computadora y procesamiento del lenguaje natural."""
        },
        {
            "title": "Transformers en NLP",
            "content": """Los modelos Transformer como BERT, GPT y RoBERTa han revolucionado el
            procesamiento del lenguaje natural. Utilizan mecanismos de atenci√≥n para capturar
            relaciones a largo plazo en el texto."""
        },
        {
            "title": "Embeddings y Representaci√≥n Vectorial",
            "content": """Los embeddings son representaciones vectoriales densas que capturan
            el significado sem√°ntico del texto. Permiten medir similitudes y realizar
            b√∫squedas sem√°nticas eficientes en grandes colecciones de documentos."""
        }
    ]
    
    # Procesar documentos
    processor = DocumentProcessorRoBERTa(use_roberta=True)
    all_chunks = []
    
    for doc in documents:
        chunks = processor.process_single_text(
            doc["content"],
            metadata={"title": doc["title"]}
        )
        all_chunks.extend(chunks)
    
    # Almacenar en MongoDB
    success = vector_store.add_documents(all_chunks)
    if success:
        print(f"‚úì {len(all_chunks)} chunks almacenados en MongoDB\n")
    
    # 4. Realizar b√∫squedas de ejemplo
    queries = [
        "¬øQu√© son los transformers en inteligencia artificial?",
        "Expl√≠came c√≥mo funcionan los embeddings",
        "Diferencia entre machine learning y deep learning"
    ]
    
    print("B√∫squedas de ejemplo:")
    print("-" * 40)
    
    for query in queries:
        print(f"\nQuery: '{query}'")
        
        # B√∫squeda por similitud
        results = vector_store.similarity_search(
            query=query,
            top_k=2,
            similarity_threshold=0.5
        )
        
        print(f"Encontrados {len(results)} resultados relevantes:")
        for i, result in enumerate(results, 1):
            title = result.get("metadata", {}).get("title", "Sin t√≠tulo")
            similarity = result.get("similarity", 0)
            content_preview = result["content"][:100] + "..."
            print(f"  {i}. [{similarity:.3f}] {title}")
            print(f"     {content_preview}")
    
    # 5. Mostrar estad√≠sticas
    stats = vector_store.get_collection_stats()
    print("\n" + "-" * 40)
    print("Estad√≠sticas de la colecci√≥n:")
    print(f"  ‚Ä¢ Total documentos: {stats['total_documents']}")
    print(f"  ‚Ä¢ IDs √∫nicos: {stats['unique_doc_ids']}")
    print(f"  ‚Ä¢ Modelos de embedding: {stats['embedding_models']}")
    print(f"  ‚Ä¢ Tama√±o promedio de chunk: {stats['average_chunk_size']:.0f} caracteres")


def compare_with_sentence_transformers():
    """Compara RoBERTa con SentenceTransformers."""
    
    print("\n" + "="*60)
    print("COMPARACI√ìN: RoBERTa vs SentenceTransformers")
    print("="*60 + "\n")
    
    sample_text = """
    Los modelos de lenguaje pre-entrenados han revolucionado el procesamiento
    del lenguaje natural. Estos modelos aprenden representaciones ricas del
    lenguaje a partir de grandes cantidades de texto no etiquetado.
    """
    
    # Procesar con RoBERTa
    print("1. Procesando con RoBERTa-base...")
    start_time = time.time()
    processor_roberta = DocumentProcessorRoBERTa(
        use_roberta=True,
        embedding_model="roberta-base"
    )
    chunks_roberta = processor_roberta.process_single_text(sample_text)
    roberta_time = time.time() - start_time
    
    # Procesar con SentenceTransformers
    print("2. Procesando con SentenceTransformers...")
    start_time = time.time()
    processor_st = DocumentProcessorRoBERTa(
        use_roberta=False,
        embedding_model="all-MiniLM-L6-v2"
    )
    chunks_st = processor_st.process_single_text(sample_text)
    st_time = time.time() - start_time
    
    # Comparar resultados
    print("\n" + "-" * 40)
    print("Resultados de la comparaci√≥n:")
    print("-" * 40)
    print(f"\nRoBERTa-base:")
    print(f"  ‚Ä¢ Dimensi√≥n de embeddings: {chunks_roberta[0]['embedding_dimension']}")
    print(f"  ‚Ä¢ Tiempo de procesamiento: {roberta_time:.3f}s")
    print(f"  ‚Ä¢ Modelo base: BERT mejorado")
    
    print(f"\nSentenceTransformers (all-MiniLM-L6-v2):")
    print(f"  ‚Ä¢ Dimensi√≥n de embeddings: {chunks_st[0]['embedding_dimension']}")
    print(f"  ‚Ä¢ Tiempo de procesamiento: {st_time:.3f}s")
    print(f"  ‚Ä¢ Modelo base: Optimizado para similitud")
    
    print("\n" + "-" * 40)
    print("Recomendaciones:")
    print("-" * 40)
    print("‚Ä¢ RoBERTa-base: Mejor para tareas que requieren comprensi√≥n profunda del contexto")
    print("‚Ä¢ SentenceTransformers: M√°s r√°pido y eficiente para b√∫squeda por similitud")
    print("‚Ä¢ La elecci√≥n depende de tu caso de uso espec√≠fico y recursos disponibles")


def process_markdown_documents():
    """Procesa documentos Markdown reales si existen."""
    
    print("\n" + "="*60)
    print("PROCESAMIENTO DE DOCUMENTOS MARKDOWN")
    print("="*60 + "\n")
    
    # Buscar directorio de documentos
    docs_dir = Path("documents")
    
    if not docs_dir.exists():
        print("‚ÑπÔ∏è  Creando directorio de documentos de ejemplo...")
        docs_dir.mkdir(exist_ok=True)
        
        # Crear documento de ejemplo
        example_doc = docs_dir / "ejemplo_roberta.md"
        example_content = """# RoBERTa: A Robustly Optimized BERT Pretraining Approach

## Introducci√≥n

RoBERTa es una reimplementaci√≥n optimizada de BERT que modifica aspectos clave del entrenamiento:

1. **Entrenamiento m√°s largo**: M√°s √©pocas con batches m√°s grandes
2. **Eliminaci√≥n de NSP**: Remueve la tarea de Next Sentence Prediction
3. **M√°scaras din√°micas**: Cambia el patr√≥n de m√°scara en cada √©poca
4. **Datos m√°s grandes**: Utiliza 10 veces m√°s datos que BERT original

## Arquitectura

RoBERTa mantiene la misma arquitectura que BERT:
- **roberta-base**: 12 capas, 768 dimensiones hidden, 12 attention heads
- **roberta-large**: 24 capas, 1024 dimensiones hidden, 16 attention heads

## Aplicaciones en RAG

En sistemas RAG, RoBERTa puede usarse para:

### 1. Generaci√≥n de Embeddings
Los embeddings de RoBERTa capturan informaci√≥n sem√°ntica rica que es √∫til para:
- B√∫squeda por similitud
- Clustering de documentos
- Detecci√≥n de duplicados

### 2. Comprensi√≥n de Consultas
RoBERTa puede entender mejor las intenciones detr√°s de las consultas de los usuarios.

### 3. Reranking
Usar RoBERTa para reordenar resultados bas√°ndose en relevancia sem√°ntica.

## Ventajas sobre BERT

- **Mejor rendimiento**: Supera a BERT en la mayor√≠a de benchmarks
- **M√°s robusto**: Menos sensible a hiperpar√°metros
- **Transferencia efectiva**: Se adapta bien a tareas downstream

## Consideraciones de Implementaci√≥n

- **Recursos computacionales**: Requiere GPU para procesamiento eficiente
- **Tama√±o del modelo**: ~125M par√°metros para roberta-base
- **Velocidad de inferencia**: M√°s lento que modelos distilados pero m√°s preciso
"""
        
        with open(example_doc, 'w', encoding='utf-8') as f:
            f.write(example_content)
        
        print(f"‚úì Creado documento de ejemplo: {example_doc}\n")
    
    # Procesar documentos
    processor = DocumentProcessorRoBERTa(
        use_roberta=True,
        embedding_model="roberta-base",
        chunk_size=500,
        chunk_overlap=100
    )
    
    # Procesar todos los archivos Markdown
    all_chunks = processor.process_documents(
        str(docs_dir),
        file_pattern="*.md",
        show_progress=True
    )
    
    if all_chunks:
        print(f"\n‚úì Procesados {len(all_chunks)} chunks de documentos Markdown")
        
        # Almacenar en MongoDB
        vector_store = MongoVectorStoreRoBERTa(
            mongodb_uri=settings.mongodb_uri,
            database_name=settings.mongodb_database,
            collection_name="markdown_roberta_collection",
            use_roberta=True
        )
        
        vector_store.clear_collection()
        success = vector_store.add_documents(all_chunks)
        
        if success:
            print(f"‚úì Chunks almacenados en MongoDB")
            
            # Realizar b√∫squeda de prueba
            test_query = "ventajas de RoBERTa sobre BERT"
            results = vector_store.similarity_search(test_query, top_k=3)
            
            print(f"\nüìç B√∫squeda de prueba: '{test_query}'")
            print(f"   Encontrados {len(results)} resultados relevantes")
    else:
        print("‚ö†Ô∏è  No se encontraron documentos Markdown para procesar")


def main():
    """Funci√≥n principal que ejecuta todas las demos."""
    
    print("\n" + "="*70)
    print(" SISTEMA RAG CON RoBERTa-base EMBEDDINGS ")
    print("="*70)
    print("\nEste script demuestra el uso de RoBERTa para generar embeddings")
    print("en un sistema RAG (Retrieval-Augmented Generation).\n")
    
    try:
        # Verificar conexi√≥n a MongoDB
        test_store = MongoVectorStoreRoBERTa(
            mongodb_uri=settings.mongodb_uri,
            database_name=settings.mongodb_database,
            collection_name="test",
            use_roberta=True
        )
        
        if not test_store.health_check():
            print("‚ö†Ô∏è  MongoDB no est√° disponible. Por favor, inicia MongoDB primero.")
            print("   Puedes iniciarlo con: brew services start mongodb-community")
            return
        
        print("‚úì Conexi√≥n a MongoDB verificada\n")
        
        # Ejecutar demos
        input("Presiona Enter para comenzar con la demo de procesamiento...")
        chunks = demo_roberta_processing()
        
        input("\nPresiona Enter para continuar con la demo del vector store...")
        demo_vector_store()
        
        input("\nPresiona Enter para ver la comparaci√≥n con SentenceTransformers...")
        compare_with_sentence_transformers()
        
        input("\nPresiona Enter para procesar documentos Markdown...")
        process_markdown_documents()
        
        print("\n" + "="*70)
        print(" DEMO COMPLETADA EXITOSAMENTE ")
        print("="*70)
        print("\n‚úÖ Todas las funcionalidades han sido demostradas.")
        print("üìö Ahora puedes usar RoBERTa en tu sistema RAG.\n")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Demo interrumpida por el usuario.")
    except Exception as e:
        logger.error(f"Error en la demo: {e}", exc_info=True)
        print(f"\n‚ùå Error: {e}")


if __name__ == "__main__":
    main()
